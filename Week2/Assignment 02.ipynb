{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "\n",
    "Title: 2.2 Exercises\n",
    "\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "\n",
    "Date: 11 Dec 2021\n",
    "\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "\n",
    "Description: This program demonstrates the creation of an Awesome Big Data stack, and using Jinja2 to format a markdown file with it.\n",
    "\n",
    "\n",
    "=========================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Big Data Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignment 2\n",
    "\n",
    "Big data is a rapidly evolving, ever-changing field. Keeping track of the latest big data stacks, programming libraries, software, and other tools requires constant vigilance. Any book on big data will be out of date by the time it is published. We need a resource that is updated on a more frequent basis. \n",
    "\n",
    "This assignment will help create that resource by researching the latest big data tools and technologies. We will use this research to create an *Awesome Big Data* list. Below is a list of similar *awesome* lists that may be useful when creating our *Awesome Big Data* list. \n",
    "\n",
    "*[Awesome Python](https://awesome-python.com/)* is a curated list of awesome Python frameworks, libraries, software and resources. It was inspired by [awesome-php](https://github.com/ziadoz/awesome-php). \n",
    "\n",
    "*[Awesome Jupyter](https://github.com/markusschanta/awesome-jupyter)* is a curated list of awesome Jupyter projects, libraries and resources. Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.\n",
    "\n",
    "*[Awesome Dash](https://github.com/ucg8j/awesome-dash)* is a curated list of awesome Dash (plotly) resources. Dash is a productive Python framework for building web applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python.\n",
    "\n",
    "*[Awesome JavaScript](https://github.com/sorrycc/awesome-javascript)* is a collection of awesome browser-side JavaScript libraries, resources and shiny things. The [data visualization section](https://github.com/sorrycc/awesome-javascript#data-visualization) may be of use. \n",
    "\n",
    "*[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)* is a curated list of awesome Deep Learning tutorials, projects and communities.\n",
    "\n",
    "*[Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)* is a curated list of awesome machine learning frameworks, libraries and software (by language).\n",
    "\n",
    "*[Awesome Data Engineering](https://github.com/igorbarinov/awesome-data-engineering)* is a curated list of data engineering tools for software developers. \n",
    "\n",
    "*[Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)* is a list of a topic-centric public data sources in high quality. They are collected and tidied from blogs, answers, and user responses. \n",
    "\n",
    "*[Awesome](https://github.com/sindresorhus/awesome)* is a list of awesome lists about all kinds of interesting topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.1\n",
    "\n",
    "Before we get started, we will access your knowledge of big data by taking the [Pokémon or Big Data Quiz](http://pixelastic.github.io/pokemonorbigdata/). Don't worry. The quiz results won't impact your grade. \n",
    "\n",
    "Included below is code that fetches the answers to the questions and provides the results in a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quiz_answers_json = 'https://raw.githubusercontent.com/pixelastic/pokemonorbigdata/master/app/questions.json'\n",
    "df_all = pd.read_json(quiz_answers_json)\n",
    "# Pokémon answers\n",
    "df_all[df_all['type'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big data answers\n",
    "df_all[df_all['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.2\n",
    "\n",
    "In the next part of the assignment, you will populate the items with categories for our list. The first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*, provides list of categories and subcategories for the big data stack. We will use these categories as a starting point, but will not constrain ourselves to them. \n",
    "\n",
    "When creating categories, avoid deeply nested layers of categories and subcategories. At most, define a top-level category with multiple subcategories. We will start with the following high-level categories and subcategories. \n",
    "\n",
    "***Categories***\n",
    "\n",
    "We will use the disutils trove classification convention defined in [PEP 301](https://www.python.org/dev/peps/pep-0301/) when defining a category with a subcategory. This convention uses the double-colon (\"::\") to separate a category and subcategory. The following is an example of categories and subcategories as defined in the first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*. \n",
    "\n",
    "- Batch Analysis :: DAG\n",
    "- Batch Analysis :: Machine Learning\n",
    "- Batch Analysis :: MapReduce\n",
    "- Batch Analysis :: Script\n",
    "- Batch Analysis :: Search\n",
    "- Batch Analysis :: Workflow Scheduling\n",
    "- Data Access Connector :: Custom Connectors\n",
    "- Data Access Connector :: Publish-Subscribe\n",
    "- Data Access Connector :: Queues\n",
    "- Data Access Connector :: SQL\n",
    "- Data Access Connector :: Source-Sink\n",
    "- Data Storage :: Distributed File System\n",
    "- Data Storage :: NoSQL\n",
    "- Deployment :: NoSQL\n",
    "- Deployment :: SQL\n",
    "- Deployment :: Visualization Frameworks\n",
    "- Deployment :: Web Frameworks\n",
    "- Interactive Querying :: Analytic SQL\n",
    "- Real-Time Analysis :: In-Memory\n",
    "- Real-Time Analysis :: Stream Processing\n",
    "\n",
    "Below is a list containing categories and suggested starting points for research. Fill in a least two items from each of the suggested categories. Create at least one category that is not listed and add two items to that category. \n",
    "\n",
    "* AI and Machine Learning\n",
    "    * Apache Spark's MLlib\n",
    "    * H2O\n",
    "    * Tensorflow\n",
    "* Batch Processing\n",
    "    * Apache\n",
    "    * Apache Spark\n",
    "    * Dask\n",
    "    * MapReduce\n",
    "* Cloud and Data Platforms\n",
    "    * Amazon Web Services\n",
    "    * Cloudera Data Platform\n",
    "    * Google Cloud Platform\n",
    "    * Microsoft Azure\n",
    "* Container Engines and Orchestration\n",
    "    * Docker\n",
    "    * Docker Swarm\n",
    "    * Kubernetes\n",
    "    * Podman\n",
    "* Data Storage :: Block Storage\n",
    "    * Amazon EBS\n",
    "    * OpenEBS\n",
    "* Data Storage :: Cluster Storage\n",
    "    * Ceph\n",
    "    * HDFS\n",
    "* Data Storage :: Object Storage\n",
    "    * Amazon S3\n",
    "    * Minio\n",
    "* Data Transfer Tools\n",
    "    * Apache Sqoop\n",
    "* Full-Text Search\n",
    "    * Apache Solr\n",
    "    * Elasticsearch\n",
    "* Interactive Query\n",
    "    * Apache Hive\n",
    "    * Google Big Query\n",
    "    * Spark SQL\n",
    "* Message Queues\n",
    "    * Apache Kafka\n",
    "    * RabbitMQ\n",
    "* NoSQL :: Document Databases\n",
    "    * CouchDB\n",
    "    * Google Firestore\n",
    "    * MongoDB\n",
    "* NoSQL :: Graph Databases\n",
    "    * DGraph\n",
    "    * Neo4j\n",
    "* NoSQL :: Key-Value Databases\n",
    "    * Amazon DynamoDB\n",
    "* NoSQL :: Time-Series Databases\n",
    "    * TSDB\n",
    "* Serverless Functions\n",
    "    * AWS Lambda\n",
    "    * OpenFaaS\n",
    "* Stream Processing\n",
    "    * Apache Spark's Structured Streaming\n",
    "    * Apache Storm\n",
    "    * Google Dataflow\n",
    "* Visualization Frameworks\n",
    "    * Apache Superset\n",
    "    * Redash\n",
    "* Workflow Engine\n",
    "    * Apache Airflow\n",
    "    * Google Cloud Composer\n",
    "    * Oozie\n",
    "    \n",
    "We populate the list items using the `ListItem` class, defined below. The following is a description of the `ListItem` fields. \n",
    "\n",
    "**name**\n",
    "\n",
    "The proper name of the list item\n",
    "\n",
    "**website**\n",
    "\n",
    "Link to the item's website.  Include `http://` or `https://` in the link. \n",
    "\n",
    "**category**\n",
    "\n",
    "Category and optional subcategory for the item. \n",
    "\n",
    "**short_description**\n",
    "\n",
    "Provide a short, one to two-sentence description of the item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ListItem:\n",
    "    name: str\n",
    "    website: str\n",
    "    category: str\n",
    "    short_description: str\n",
    "    \n",
    "all_items = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of creating the entry for AWS as a seperate variable and then adding it to the `all_items` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws = ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    ")\n",
    "\n",
    "all_items.add(aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add an item to the list directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items.remove(aws)\n",
    "all_items.add(ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in a least two items from each of the suggested categories. \n",
    "\n",
    "# TODO: Create at least one category that is not listed and add two items to that category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List allows for future updates to all_items, using next cell\n",
    "list_entries = [\n",
    "    # AI and Machine Learning\n",
    "    ListItem(\n",
    "        \"Apache Spark's MLlib\",\n",
    "        'https://spark.apache.org/mllib/',\n",
    "        'AI and Machine Learning',\n",
    "        \"\"\"MLlib is Apache Spark's scalable machine learning library. Ease of use. Usable in Java, Scala, Python, and R.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'H2O',\n",
    "        'https://www.h2o.ai/',\n",
    "        'AI and Machine Learning',\n",
    "        \"\"\"H2O.ai is an advanced AI Cloud Platform designed to simplify and accelerate making, operating and innovating with AI in any environment.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Tensorflow',\n",
    "        'https://www.tensorflow.org/',\n",
    "        'AI and Machine Learning',\n",
    "        \"\"\"TensorFlow is a free and open-source software library for machine learning and artificial intelligence.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Batch Processing\n",
    "    ListItem(\n",
    "        'Apache Beam',\n",
    "        'https://beam.apache.org/',\n",
    "        'Batch Processing',\n",
    "        \"\"\"Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Apache Spark',\n",
    "        'https://spark.apache.org/',\n",
    "        'Batch Processing',\n",
    "        \"\"\"Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Dask',\n",
    "        'https://dask.org/',\n",
    "        'Batch Processing',\n",
    "        \"\"\"Dask is an open-source flexible parallel computing library written in Python for analytics\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Cloud and Data Platforms\n",
    "    ListItem(\n",
    "        'Amazon Web Services',\n",
    "        'https://aws.amazon.com/',\n",
    "        'Cloud and Data Platforms',\n",
    "        \"\"\"Amazon Web Services, Inc. is a subsidiary of Amazon providing on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Cloudera Data Platform',\n",
    "        'https://www.cloudera.com/products/cloudera-data-platform.html',\n",
    "        'Cloud and Data Platforms',\n",
    "        \"\"\"Cloudera’s open-source data platform uses analytics and machine learning to yield insights from data through a secure connection.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Google Cloud Platform',\n",
    "        'https://cloud.google.com/',\n",
    "        'Cloud and Data Platforms',\n",
    "        \"\"\"Google Cloud Platform, offered by Google, is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Microsoft Azure',\n",
    "        'https://azure.microsoft.com/',\n",
    "        'Cloud and Data Platforms',\n",
    "        \"\"\"Microsoft Azure, often referred to as Azure, is a cloud computing service operated by Microsoft for application management via Microsoft-managed data centers.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Container Engines and Orchestration\n",
    "    ListItem(\n",
    "        'Docker',\n",
    "        'https://www.docker.com/',\n",
    "        'Container Engines and Orchestration',\n",
    "        \"\"\"Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Kubernetes',\n",
    "        'https://kubernetes.io/',\n",
    "        'Container Engines and Orchestration',\n",
    "        \"\"\"Kubernetes is an open-source container-orchestration system for automating computer application deployment, scaling, and management.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Podman',\n",
    "        'https://podman.io/',\n",
    "        'Container Engines and Orchestration',\n",
    "        \"\"\"Podman is a daemonless, open source, Linux native tool designed to make it easy to find, run, build, share and deploy applications using Open Containers Initiative (OCI) Containers and Container Images.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Data Storage :: Block Storage\n",
    "    ListItem(\n",
    "        'Amazon EBS',\n",
    "        'https://aws.amazon.com/ebs/',\n",
    "        'Data Storage :: Block Storage',\n",
    "        \"\"\"Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud (Amazon EC2).\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'OpenEBS',\n",
    "        'https://openebs.io/',\n",
    "        'Data Storage :: Block Storage',\n",
    "        \"\"\"OpenESB is a Java-based open-source enterprise service bus. It allows you to integrate legacy systems, external and internal partners and new development in your Business Process.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Data Storage :: Cluster Storage\n",
    "    ListItem(\n",
    "        'Ceph',\n",
    "        'https://ceph.io/en/',\n",
    "        'Data Storage :: Cluster Storage',\n",
    "        \"\"\"Ceph is an open-source software storage platform, implements object storage on a single distributed computer cluster, and provides 3-in-1 interfaces for object-, block- and file-level storage.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Hadoop Distributed File System',\n",
    "        'https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html',\n",
    "        'Data Storage :: Cluster Storage',\n",
    "        \"\"\"The Hadoop Distributed File System ( HDFS ) is a distributed file system designed to run on commodity hardware.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Data Storage :: Object Storage\n",
    "    ListItem(\n",
    "        'Amazon S3',\n",
    "        'https://aws.amazon.com/s3/',\n",
    "        'Data Storage :: Object Storage',\n",
    "        \"\"\"Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services that provides scalable object storage through a web service interface.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Minio',\n",
    "        'https://min.io/',\n",
    "        'Data Storage :: Object Storage',\n",
    "        \"\"\"MinIO is a High Performance Object Storage that is API compatible with Amazon S3 cloud storage service. It can handle unstructured data such as photos, videos, log files, backups, and container images with the maximum supported object size of 5TB.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Data Transfer Tools\n",
    "    ListItem(\n",
    "        'Apache Sqoop',\n",
    "        'https://sqoop.apache.org/',\n",
    "        'Data Transfer Tools',\n",
    "        \"\"\"Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. The Apache Sqoop project was retired in June 2021 and moved to the Apache Attic.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Full-Text Search\n",
    "    ListItem(\n",
    "        'Apache Solr',\n",
    "        'https://solr.apache.org/',\n",
    "        'Full-Text Search',\n",
    "        \"\"\"Solr is an open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features and rich document handling.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Elasticsearch',\n",
    "        'https://www.elastic.co/elasticsearch/',\n",
    "        'Full-Text Search',\n",
    "        \"\"\"Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Interactive Query\n",
    "    ListItem(\n",
    "        'Apache Hive',\n",
    "        'https://hive.apache.org/',\n",
    "        'Interactive Query',\n",
    "        \"\"\"Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Google Big Query',\n",
    "        'https://cloud.google.com/bigquery',\n",
    "        'Interactive Query',\n",
    "        \"\"\"BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Spark SQL',\n",
    "        'https://spark.apache.org/sql/',\n",
    "        'Interactive Query',\n",
    "        \"\"\"Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Message Queues\n",
    "    ListItem(\n",
    "        'Apache Kafka',\n",
    "        'https://kafka.apache.org/',\n",
    "        'Message Queues',\n",
    "        \"\"\"Apache Kafka is an open-source framework implementation of a software bus using stream-processing. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'RabbitMQ',\n",
    "        'https://www.rabbitmq.com/',\n",
    "        'Message Queues',\n",
    "        \"\"\"RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol, MQ Telemetry Transport, and other protocols.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # NoSQL :: Document Databases\n",
    "    ListItem(\n",
    "        'CouchDB',\n",
    "        'https://couchdb.apache.org/',\n",
    "        'NoSQL :: Document Databases',\n",
    "        \"\"\"Apache CouchDB is an open-source document-oriented NoSQL database, implemented in Erlang. CouchDB uses multiple formats and protocols to store, transfer, and process its data. It uses JSON to store data, JavaScript as its query language using MapReduce, and HTTP for an API.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Google Firestore',\n",
    "        'https://cloud.google.com/firestore',\n",
    "        'NoSQL :: Document Databases',\n",
    "        \"\"\"Firebase is a platform developed by Google for creating mobile and web applications. It allows you to run sophisticated ACID transactions against your document data.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'MongoDB',\n",
    "        'https://www.mongodb.com/atlas',\n",
    "        'NoSQL :: Document Databases',\n",
    "        \"\"\"MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # NoSQL :: Graph Databases\n",
    "    ListItem(\n",
    "        'DGraph',\n",
    "        'https://dgraph.io/',\n",
    "        'NoSQL :: Graph Databases',\n",
    "        \"\"\"Dgraph is a open-source graph database management system. Dgraph uses Raft for shard replication and a custom transactional protocol for snapshot-isolated cross-shard transactions.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Neo4j',\n",
    "        'https://neo4j.com/product/neo4j-graph-database/',\n",
    "        'NoSQL :: Graph Databases',\n",
    "        \"\"\"Neo4j is a graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing,\"\"\"\n",
    "    ),\n",
    "\n",
    "    # NoSQL :: Key-Value Databases\n",
    "    ListItem(\n",
    "        'Amazon DynamoDB',\n",
    "        'https://aws.amazon.com/dynamodb/',\n",
    "        'NoSQL :: Key-Value Databases',\n",
    "        \"\"\"Amazon DynamoDB is a fully managed proprietary NoSQL database service that supports key–value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # NoSQL :: Time-Series Databases\n",
    "    ListItem(\n",
    "        'OpenTSDB',\n",
    "        'http://opentsdb.net/',\n",
    "        'NoSQL :: Time-Series Databases',\n",
    "        \"\"\"OpenTSDB is a distributed, scalable Time Series Database (TSDB) written on top of HBase. OpenTSDB was written to address a common need: store, index and serve metrics collected from computer systems at a large scale, and make this data easily accessible and graphable.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Serverless Functions\n",
    "    ListItem(\n",
    "        'AWS Lambda',\n",
    "        'https://aws.amazon.com/lambda/',\n",
    "        'Serverless Functions',\n",
    "        \"\"\"AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'OpenFaaS',\n",
    "        'https://www.openfaas.com/',\n",
    "        'Serverless Functions',\n",
    "        \"\"\"OpenFaaS is an open source serverless function engine where users can publish, run, and manage functions on Kubernetes clusters.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Stream Processing\n",
    "    ListItem(\n",
    "        'Apache Storm',\n",
    "        'https://storm.apache.org/',\n",
    "        'Stream Processing',\n",
    "        \"\"\"Apache Storm is an open-source distributed stream processing computation framework written predominantly in the Clojure programming language.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Google Dataflow',\n",
    "        'https://cloud.google.com/dataflow',\n",
    "        'Stream Processing',\n",
    "        \"\"\"Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Version Control Systems\n",
    "    ListItem(\n",
    "        'Data Version Control',\n",
    "        'https://dvc.org/',\n",
    "        'Version Control Systems',\n",
    "        \"\"\"DVC is an open-source version control system for machine learning projects that lets you define your pipeline regarless of the language used.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Git LFS',\n",
    "        'https://git-lfs.github.com/',\n",
    "        'Version Control Systems',\n",
    "        \"\"\"Git Large File Storage (LFS) is an open-source project that allows you to version large files with Git.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Visualization Frameworks\n",
    "    ListItem(\n",
    "        'Apache Superset',\n",
    "        'https://superset.apache.org/',\n",
    "        'Visualization Frameworks',\n",
    "        \"\"\"Apache Superset is an open-source software cloud-native application for data exploration and data visualization able to handle data at petabyte scale.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Redash',\n",
    "        'https://redash.io/',\n",
    "        'Visualization Frameworks',\n",
    "        \"\"\"Redash is an open-source tool for teams to query, visualize and collaborate.\"\"\"\n",
    "    ),\n",
    "\n",
    "    # Workflow Engine\n",
    "    ListItem(\n",
    "        'Apache Airflow',\n",
    "        'https://airflow.apache.org/',\n",
    "        'Workflow Engine',\n",
    "        \"\"\"Apache Airflow is an open-source workflow management platform for data engineering pipelines.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Google Cloud Composer',\n",
    "        'https://cloud.google.com/composer',\n",
    "        'Workflow Engine',\n",
    "        \"\"\"Cloud Composer is a managed workflow automation tool that is built on Apache Airflow. It's used to author, schedule, and monitor software development pipelines across data centers.\"\"\"\n",
    "    ),\n",
    "\n",
    "    ListItem(\n",
    "        'Oozie',\n",
    "        'https://oozie.apache.org/',\n",
    "        'Workflow Engine',\n",
    "        \"\"\"Apache Oozie is a server-based workflow scheduling system to manage Hadoop jobs. Workflows in Oozie are defined as a collection of control flow and action nodes in a directed acyclic graph.\"\"\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Web Services already in all_items\n",
      "Update? (Y/N)Y\n"
     ]
    }
   ],
   "source": [
    "items = [item.name for item in all_items]\n",
    "\n",
    "# Adds each element to all_items\n",
    "for entry in list_entries:\n",
    "    if entry.name in items:\n",
    "        print(f'{entry.name} already in all_items')\n",
    "        \n",
    "        # If element name already exists, offers to update in all_items\n",
    "        if input('Update? (Y/N)').upper() == 'Y':\n",
    "            outdated = next((item for item in all_items if item.name == entry.name), None)\n",
    "            all_items.remove(outdated)\n",
    "            all_items.add(entry)\n",
    "\n",
    "    else:\n",
    "        all_items.add(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.3 (Optional)\n",
    "\n",
    "Use the `all_items` data to create Markdown output that mirrors the output of [Awesome Python](https://raw.githubusercontent.com/vinta/awesome-python/master/README.md). You can use the `jinja2` template engine to complete this task. This part of the assignment is entirely optional and is not graded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "template = jinja2.Template(\"\"\"\n",
    "# Awesome Big Data\n",
    "\n",
    "{% for category, items in category_dict.items() %}\n",
    "## {{category}}\n",
    "\n",
    "{% for item in items: %}\n",
    "* [{{item.name}}]({{item.website}}) - {{item.short_description}}\n",
    "\n",
    "{% endfor %} \n",
    "{% endfor %}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "categories = set(item.category for item in all_items)\n",
    "\n",
    "# Creates a k,v pair for categories and each item in category\n",
    "category_dict = defaultdict(list)\n",
    "for item in all_items:\n",
    "    for category in categories:\n",
    "        if item.category == category:\n",
    "            category_dict[category].append(item)\n",
    "            \n",
    "# Orders the dictionary because it's better that way... Trust me.\n",
    "category_dict = OrderedDict(sorted(category_dict.items(), key=lambda t: t[0]))\n",
    "\n",
    "# context just nests the dict; \n",
    "# jinja2.Template cant see highest level of *arg for some reason\n",
    "context = {'category_dict': category_dict}\n",
    "\n",
    "markdown_result = template.render(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Awesome Big Data\n",
       "\n",
       "\n",
       "## AI and Machine Learning\n",
       "\n",
       "\n",
       "* [Tensorflow](https://www.tensorflow.org/) - TensorFlow is a free and open-source software library for machine learning and artificial intelligence.\n",
       "\n",
       "\n",
       "* [Apache Spark's MLlib](https://spark.apache.org/mllib/) - MLlib is Apache Spark's scalable machine learning library. Ease of use. Usable in Java, Scala, Python, and R.\n",
       "\n",
       "\n",
       "* [H2O](https://www.h2o.ai/) - H2O.ai is an advanced AI Cloud Platform designed to simplify and accelerate making, operating and innovating with AI in any environment.\n",
       "\n",
       " \n",
       "\n",
       "## Batch Processing\n",
       "\n",
       "\n",
       "* [Apache Beam](https://beam.apache.org/) - Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing\n",
       "\n",
       "\n",
       "* [Apache Spark](https://spark.apache.org/) - Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
       "\n",
       "\n",
       "* [Dask](https://dask.org/) - Dask is an open-source flexible parallel computing library written in Python for analytics\n",
       "\n",
       " \n",
       "\n",
       "## Cloud and Data Platforms\n",
       "\n",
       "\n",
       "* [Microsoft Azure](https://azure.microsoft.com/) - Microsoft Azure, often referred to as Azure, is a cloud computing service operated by Microsoft for application management via Microsoft-managed data centers.\n",
       "\n",
       "\n",
       "* [Cloudera Data Platform](https://www.cloudera.com/products/cloudera-data-platform.html) - Cloudera’s open-source data platform uses analytics and machine learning to yield insights from data through a secure connection.\n",
       "\n",
       "\n",
       "* [Amazon Web Services](https://aws.amazon.com/) - Amazon Web Services, Inc. is a subsidiary of Amazon providing on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis.\n",
       "\n",
       "\n",
       "* [Google Cloud Platform](https://cloud.google.com/) - Google Cloud Platform, offered by Google, is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube.\n",
       "\n",
       " \n",
       "\n",
       "## Container Engines and Orchestration\n",
       "\n",
       "\n",
       "* [Kubernetes](https://kubernetes.io/) - Kubernetes is an open-source container-orchestration system for automating computer application deployment, scaling, and management.\n",
       "\n",
       "\n",
       "* [Podman](https://podman.io/) - Podman is a daemonless, open source, Linux native tool designed to make it easy to find, run, build, share and deploy applications using Open Containers Initiative (OCI) Containers and Container Images.\n",
       "\n",
       "\n",
       "* [Docker](https://www.docker.com/) - Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers.\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Block Storage\n",
       "\n",
       "\n",
       "* [OpenEBS](https://openebs.io/) - OpenESB is a Java-based open-source enterprise service bus. It allows you to integrate legacy systems, external and internal partners and new development in your Business Process.\n",
       "\n",
       "\n",
       "* [Amazon EBS](https://aws.amazon.com/ebs/) - Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud (Amazon EC2).\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Cluster Storage\n",
       "\n",
       "\n",
       "* [Hadoop Distributed File System](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) - The Hadoop Distributed File System ( HDFS ) is a distributed file system designed to run on commodity hardware.\n",
       "\n",
       "\n",
       "* [Ceph](https://ceph.io/en/) - Ceph is an open-source software storage platform, implements object storage on a single distributed computer cluster, and provides 3-in-1 interfaces for object-, block- and file-level storage.\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Object Storage\n",
       "\n",
       "\n",
       "* [Minio](https://min.io/) - MinIO is a High Performance Object Storage that is API compatible with Amazon S3 cloud storage service. It can handle unstructured data such as photos, videos, log files, backups, and container images with the maximum supported object size of 5TB.\n",
       "\n",
       "\n",
       "* [Amazon S3](https://aws.amazon.com/s3/) - Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services that provides scalable object storage through a web service interface.\n",
       "\n",
       " \n",
       "\n",
       "## Data Transfer Tools\n",
       "\n",
       "\n",
       "* [Apache Sqoop](https://sqoop.apache.org/) - Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. The Apache Sqoop project was retired in June 2021 and moved to the Apache Attic.\n",
       "\n",
       " \n",
       "\n",
       "## Full-Text Search\n",
       "\n",
       "\n",
       "* [Elasticsearch](https://www.elastic.co/elasticsearch/) - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\n",
       "\n",
       "\n",
       "* [Apache Solr](https://solr.apache.org/) - Solr is an open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features and rich document handling.\n",
       "\n",
       " \n",
       "\n",
       "## Interactive Query\n",
       "\n",
       "\n",
       "* [Spark SQL](https://spark.apache.org/sql/) - Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.\n",
       "\n",
       "\n",
       "* [Apache Hive](https://hive.apache.org/) - Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\n",
       "\n",
       "\n",
       "* [Google Big Query](https://cloud.google.com/bigquery) - BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities.\n",
       "\n",
       " \n",
       "\n",
       "## Message Queues\n",
       "\n",
       "\n",
       "* [RabbitMQ](https://www.rabbitmq.com/) - RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol, MQ Telemetry Transport, and other protocols.\n",
       "\n",
       "\n",
       "* [Apache Kafka](https://kafka.apache.org/) - Apache Kafka is an open-source framework implementation of a software bus using stream-processing. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Document Databases\n",
       "\n",
       "\n",
       "* [CouchDB](https://couchdb.apache.org/) - Apache CouchDB is an open-source document-oriented NoSQL database, implemented in Erlang. CouchDB uses multiple formats and protocols to store, transfer, and process its data. It uses JSON to store data, JavaScript as its query language using MapReduce, and HTTP for an API.\n",
       "\n",
       "\n",
       "* [MongoDB](https://www.mongodb.com/atlas) - MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.\n",
       "\n",
       "\n",
       "* [Google Firestore](https://cloud.google.com/firestore) - Firebase is a platform developed by Google for creating mobile and web applications. It allows you to run sophisticated ACID transactions against your document data.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Graph Databases\n",
       "\n",
       "\n",
       "* [DGraph](https://dgraph.io/) - Dgraph is a open-source graph database management system. Dgraph uses Raft for shard replication and a custom transactional protocol for snapshot-isolated cross-shard transactions.\n",
       "\n",
       "\n",
       "* [Neo4j](https://neo4j.com/product/neo4j-graph-database/) - Neo4j is a graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing,\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Key-Value Databases\n",
       "\n",
       "\n",
       "* [Amazon DynamoDB](https://aws.amazon.com/dynamodb/) - Amazon DynamoDB is a fully managed proprietary NoSQL database service that supports key–value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Time-Series Databases\n",
       "\n",
       "\n",
       "* [OpenTSDB](http://opentsdb.net/) - OpenTSDB is a distributed, scalable Time Series Database (TSDB) written on top of HBase. OpenTSDB was written to address a common need: store, index and serve metrics collected from computer systems at a large scale, and make this data easily accessible and graphable.\n",
       "\n",
       " \n",
       "\n",
       "## Serverless Functions\n",
       "\n",
       "\n",
       "* [AWS Lambda](https://aws.amazon.com/lambda/) - AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.\n",
       "\n",
       "\n",
       "* [OpenFaaS](https://www.openfaas.com/) - OpenFaaS is an open source serverless function engine where users can publish, run, and manage functions on Kubernetes clusters.\n",
       "\n",
       " \n",
       "\n",
       "## Stream Processing\n",
       "\n",
       "\n",
       "* [Google Dataflow](https://cloud.google.com/dataflow) - Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem.\n",
       "\n",
       "\n",
       "* [Apache Storm](https://storm.apache.org/) - Apache Storm is an open-source distributed stream processing computation framework written predominantly in the Clojure programming language.\n",
       "\n",
       " \n",
       "\n",
       "## Version Control Systems\n",
       "\n",
       "\n",
       "* [Data Version Control](https://dvc.org/) - DVC is an open-source version control system for machine learning projects that lets you define your pipeline regarless of the language used.\n",
       "\n",
       "\n",
       "* [Git LFS](https://git-lfs.github.com/) - Git Large File Storage (LFS) is an open-source project that allows you to version large files with Git.\n",
       "\n",
       " \n",
       "\n",
       "## Visualization Frameworks\n",
       "\n",
       "\n",
       "* [Redash](https://redash.io/) - Redash is an open-source tool for teams to query, visualize and collaborate.\n",
       "\n",
       "\n",
       "* [Apache Superset](https://superset.apache.org/) - Apache Superset is an open-source software cloud-native application for data exploration and data visualization able to handle data at petabyte scale.\n",
       "\n",
       " \n",
       "\n",
       "## Workflow Engine\n",
       "\n",
       "\n",
       "* [Google Cloud Composer](https://cloud.google.com/composer) - Cloud Composer is a managed workflow automation tool that is built on Apache Airflow. It's used to author, schedule, and monitor software development pipelines across data centers.\n",
       "\n",
       "\n",
       "* [Apache Airflow](https://airflow.apache.org/) - Apache Airflow is an open-source workflow management platform for data engineering pipelines.\n",
       "\n",
       "\n",
       "* [Oozie](https://oozie.apache.org/) - Apache Oozie is a server-based workflow scheduling system to manage Hadoop jobs. Workflows in Oozie are defined as a collection of control flow and action nodes in a directed acyclic graph.\n",
       "\n",
       " \n"
      ],
      "text/plain": [
       "\n",
       "# Awesome Big Data\n",
       "\n",
       "\n",
       "## AI and Machine Learning\n",
       "\n",
       "\n",
       "* [Tensorflow](https://www.tensorflow.org/) - TensorFlow is a free and open-source software library for machine learning and artificial intelligence.\n",
       "\n",
       "\n",
       "* [Apache Spark's MLlib](https://spark.apache.org/mllib/) - MLlib is Apache Spark's scalable machine learning library. Ease of use. Usable in Java, Scala, Python, and R.\n",
       "\n",
       "\n",
       "* [H2O](https://www.h2o.ai/) - H2O.ai is an advanced AI Cloud Platform designed to simplify and accelerate making, operating and innovating with AI in any environment.\n",
       "\n",
       " \n",
       "\n",
       "## Batch Processing\n",
       "\n",
       "\n",
       "* [Apache Beam](https://beam.apache.org/) - Apache Beam is an open source unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing\n",
       "\n",
       "\n",
       "* [Apache Spark](https://spark.apache.org/) - Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
       "\n",
       "\n",
       "* [Dask](https://dask.org/) - Dask is an open-source flexible parallel computing library written in Python for analytics\n",
       "\n",
       " \n",
       "\n",
       "## Cloud and Data Platforms\n",
       "\n",
       "\n",
       "* [Microsoft Azure](https://azure.microsoft.com/) - Microsoft Azure, often referred to as Azure, is a cloud computing service operated by Microsoft for application management via Microsoft-managed data centers.\n",
       "\n",
       "\n",
       "* [Cloudera Data Platform](https://www.cloudera.com/products/cloudera-data-platform.html) - Cloudera’s open-source data platform uses analytics and machine learning to yield insights from data through a secure connection.\n",
       "\n",
       "\n",
       "* [Amazon Web Services](https://aws.amazon.com/) - Amazon Web Services, Inc. is a subsidiary of Amazon providing on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis.\n",
       "\n",
       "\n",
       "* [Google Cloud Platform](https://cloud.google.com/) - Google Cloud Platform, offered by Google, is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube.\n",
       "\n",
       " \n",
       "\n",
       "## Container Engines and Orchestration\n",
       "\n",
       "\n",
       "* [Kubernetes](https://kubernetes.io/) - Kubernetes is an open-source container-orchestration system for automating computer application deployment, scaling, and management.\n",
       "\n",
       "\n",
       "* [Podman](https://podman.io/) - Podman is a daemonless, open source, Linux native tool designed to make it easy to find, run, build, share and deploy applications using Open Containers Initiative (OCI) Containers and Container Images.\n",
       "\n",
       "\n",
       "* [Docker](https://www.docker.com/) - Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers.\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Block Storage\n",
       "\n",
       "\n",
       "* [OpenEBS](https://openebs.io/) - OpenESB is a Java-based open-source enterprise service bus. It allows you to integrate legacy systems, external and internal partners and new development in your Business Process.\n",
       "\n",
       "\n",
       "* [Amazon EBS](https://aws.amazon.com/ebs/) - Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud (Amazon EC2).\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Cluster Storage\n",
       "\n",
       "\n",
       "* [Hadoop Distributed File System](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) - The Hadoop Distributed File System ( HDFS ) is a distributed file system designed to run on commodity hardware.\n",
       "\n",
       "\n",
       "* [Ceph](https://ceph.io/en/) - Ceph is an open-source software storage platform, implements object storage on a single distributed computer cluster, and provides 3-in-1 interfaces for object-, block- and file-level storage.\n",
       "\n",
       " \n",
       "\n",
       "## Data Storage :: Object Storage\n",
       "\n",
       "\n",
       "* [Minio](https://min.io/) - MinIO is a High Performance Object Storage that is API compatible with Amazon S3 cloud storage service. It can handle unstructured data such as photos, videos, log files, backups, and container images with the maximum supported object size of 5TB.\n",
       "\n",
       "\n",
       "* [Amazon S3](https://aws.amazon.com/s3/) - Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services that provides scalable object storage through a web service interface.\n",
       "\n",
       " \n",
       "\n",
       "## Data Transfer Tools\n",
       "\n",
       "\n",
       "* [Apache Sqoop](https://sqoop.apache.org/) - Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. The Apache Sqoop project was retired in June 2021 and moved to the Apache Attic.\n",
       "\n",
       " \n",
       "\n",
       "## Full-Text Search\n",
       "\n",
       "\n",
       "* [Elasticsearch](https://www.elastic.co/elasticsearch/) - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\n",
       "\n",
       "\n",
       "* [Apache Solr](https://solr.apache.org/) - Solr is an open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features and rich document handling.\n",
       "\n",
       " \n",
       "\n",
       "## Interactive Query\n",
       "\n",
       "\n",
       "* [Spark SQL](https://spark.apache.org/sql/) - Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.\n",
       "\n",
       "\n",
       "* [Apache Hive](https://hive.apache.org/) - Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\n",
       "\n",
       "\n",
       "* [Google Big Query](https://cloud.google.com/bigquery) - BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities.\n",
       "\n",
       " \n",
       "\n",
       "## Message Queues\n",
       "\n",
       "\n",
       "* [RabbitMQ](https://www.rabbitmq.com/) - RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol, MQ Telemetry Transport, and other protocols.\n",
       "\n",
       "\n",
       "* [Apache Kafka](https://kafka.apache.org/) - Apache Kafka is an open-source framework implementation of a software bus using stream-processing. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Document Databases\n",
       "\n",
       "\n",
       "* [CouchDB](https://couchdb.apache.org/) - Apache CouchDB is an open-source document-oriented NoSQL database, implemented in Erlang. CouchDB uses multiple formats and protocols to store, transfer, and process its data. It uses JSON to store data, JavaScript as its query language using MapReduce, and HTTP for an API.\n",
       "\n",
       "\n",
       "* [MongoDB](https://www.mongodb.com/atlas) - MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.\n",
       "\n",
       "\n",
       "* [Google Firestore](https://cloud.google.com/firestore) - Firebase is a platform developed by Google for creating mobile and web applications. It allows you to run sophisticated ACID transactions against your document data.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Graph Databases\n",
       "\n",
       "\n",
       "* [DGraph](https://dgraph.io/) - Dgraph is a open-source graph database management system. Dgraph uses Raft for shard replication and a custom transactional protocol for snapshot-isolated cross-shard transactions.\n",
       "\n",
       "\n",
       "* [Neo4j](https://neo4j.com/product/neo4j-graph-database/) - Neo4j is a graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing,\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Key-Value Databases\n",
       "\n",
       "\n",
       "* [Amazon DynamoDB](https://aws.amazon.com/dynamodb/) - Amazon DynamoDB is a fully managed proprietary NoSQL database service that supports key–value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio.\n",
       "\n",
       " \n",
       "\n",
       "## NoSQL :: Time-Series Databases\n",
       "\n",
       "\n",
       "* [OpenTSDB](http://opentsdb.net/) - OpenTSDB is a distributed, scalable Time Series Database (TSDB) written on top of HBase. OpenTSDB was written to address a common need: store, index and serve metrics collected from computer systems at a large scale, and make this data easily accessible and graphable.\n",
       "\n",
       " \n",
       "\n",
       "## Serverless Functions\n",
       "\n",
       "\n",
       "* [AWS Lambda](https://aws.amazon.com/lambda/) - AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.\n",
       "\n",
       "\n",
       "* [OpenFaaS](https://www.openfaas.com/) - OpenFaaS is an open source serverless function engine where users can publish, run, and manage functions on Kubernetes clusters.\n",
       "\n",
       " \n",
       "\n",
       "## Stream Processing\n",
       "\n",
       "\n",
       "* [Google Dataflow](https://cloud.google.com/dataflow) - Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem.\n",
       "\n",
       "\n",
       "* [Apache Storm](https://storm.apache.org/) - Apache Storm is an open-source distributed stream processing computation framework written predominantly in the Clojure programming language.\n",
       "\n",
       " \n",
       "\n",
       "## Version Control Systems\n",
       "\n",
       "\n",
       "* [Data Version Control](https://dvc.org/) - DVC is an open-source version control system for machine learning projects that lets you define your pipeline regarless of the language used.\n",
       "\n",
       "\n",
       "* [Git LFS](https://git-lfs.github.com/) - Git Large File Storage (LFS) is an open-source project that allows you to version large files with Git.\n",
       "\n",
       " \n",
       "\n",
       "## Visualization Frameworks\n",
       "\n",
       "\n",
       "* [Redash](https://redash.io/) - Redash is an open-source tool for teams to query, visualize and collaborate.\n",
       "\n",
       "\n",
       "* [Apache Superset](https://superset.apache.org/) - Apache Superset is an open-source software cloud-native application for data exploration and data visualization able to handle data at petabyte scale.\n",
       "\n",
       " \n",
       "\n",
       "## Workflow Engine\n",
       "\n",
       "\n",
       "* [Google Cloud Composer](https://cloud.google.com/composer) - Cloud Composer is a managed workflow automation tool that is built on Apache Airflow. It's used to author, schedule, and monitor software development pipelines across data centers.\n",
       "\n",
       "\n",
       "* [Apache Airflow](https://airflow.apache.org/) - Apache Airflow is an open-source workflow management platform for data engineering pipelines.\n",
       "\n",
       "\n",
       "* [Oozie](https://oozie.apache.org/) - Apache Oozie is a server-based workflow scheduling system to manage Hadoop jobs. Workflows in Oozie are defined as a collection of control flow and action nodes in a directed acyclic graph.\n",
       "\n",
       " \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display({'text/plain': markdown_result,\n",
    "         'text/markdown': markdown_result},\n",
    "        raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
