{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "\n",
    "Title: 9.2 Exercises\n",
    "\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "\n",
    "Date: 12 Feb 2022\n",
    "\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "\n",
    "Description: This program demonstrates the use of spark to perform clustering with LDA, K-Means, and regression models on sample data. Code implimented from sample sites, per instruction. Refrences included.\n",
    "\n",
    "=========================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9\n",
    "\n",
    "The following code initializes a Spark session that you use for the remainder of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DSC 400 Assignment 9\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# TODO: Change these to point to versions on your local path\n",
    "\n",
    "sample_kmeans_data_path = r'data\\sample_kmeans_data.txt'\n",
    "sample_lda_data_path = r'data\\sample_lda_data.txt'\n",
    "sample_movielens_data_path = r'data\\sample_movielens_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 9.1\n",
    "\n",
    "Run the PySpark version of the k-means clustering example found in [Apache Spark's k-means clustering documentation](https://spark.apache.org/docs/latest/ml-clustering.html#k-means). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_kmeans_data.txt` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_kmeans_data.txt. Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/sample_kmeans_data.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(sample_kmeans_data_path)\n",
    "\n",
    "# TODO: Implement the remainder of the code from the k-means clustering example\n",
    "\n",
    "# Code implimented from https://spark.apache.org/docs/latest/ml-clustering.html#k-means \n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 9.2\n",
    "\n",
    "Run the PySpark version of the Latent Dirichlet Allocation (LDA) example found in [Apache Spark's LDA documentation](https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_lda_libsvm_data.txt\n",
    "` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_lda_libsvm_data.txt\n",
    ". Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "dataset = dataset = spark.read.format(\"libsvm\").load(\"data/sample_lda_data.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -796.5399751899204\n",
      "The upper bound on perplexity: 3.0636152891920014\n",
      "The topics described by their top-weighted terms:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrcha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[5, 10, 8] |[0.10142142863147144, 0.10067380726498025, 0.09966345558028378]|\n",
      "|1    |[6, 1, 4]  |[0.10397121695942116, 0.10168853086587061, 0.09387262850459155]|\n",
      "|2    |[7, 0, 9]  |[0.10585830481697496, 0.10328073972555847, 0.09961278145953019]|\n",
      "|3    |[3, 5, 4]  |[0.10607015157178218, 0.10025508640011113, 0.09950695826225199]|\n",
      "|4    |[4, 5, 1]  |[0.1625975422412709, 0.14053559661747037, 0.1358456725227939]  |\n",
      "|5    |[10, 6, 9] |[0.2511233507213596, 0.15468631922794587, 0.12582321496738322] |\n",
      "|6    |[5, 0, 7]  |[0.10727666672469126, 0.09553834984523368, 0.09510957588066214]|\n",
      "|7    |[9, 0, 5]  |[0.10770202564363905, 0.10638992800158185, 0.09907350535796362]|\n",
      "|8    |[7, 3, 5]  |[0.1088447905791633, 0.10266373799409191, 0.09493293616845823] |\n",
      "|9    |[1, 9, 4]  |[0.1114455010254971, 0.10606810186006342, 0.0982723809803197]  |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                    |\n",
      "+-----+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004734950630002274,0.004734944532580187,0.004734979112348759,0.004734952729267079,0.9569165970209891,0.0052037536945325836,0.004734940044674167,0.004734977417719526,0.004734959894241789,0.004734944923644437]   |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007900194090246657,0.007900190359912547,0.007900224814920843,0.007900315712678081,0.9281162839978762,0.008681906816473737,0.007900167212850247,0.007900191898694578,0.007900294319635287,0.007900230776711835]    |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004116510282343024,0.004116557080074703,0.004116528881460988,0.004116507810239563,0.3874455859355103,0.5796221960875608,0.004116530993966133,0.0041165432718481675,0.0041165240688214285,0.004116515588174752]    |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.0036408158982518864,0.0036408108686522386,0.0036408143895234877,0.003640819166850504,0.004140517739848204,0.9667330075815687,0.003640790453105253,0.0036408226969014655,0.003640793885113629,0.003640807320184723]|\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003944822173153937,0.003944812578146465,0.003944850250194505,0.003944922312537313,0.23001995744430215,0.7384212076151497,0.00394482876446994,0.00394488023787054,0.003944875201658228,0.003944843422517229]       |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0036411092128956655,0.003641074050365259,0.003641089726111888,0.0036411503610669894,0.9668695762698701,0.0040015659702122355,0.0036410935827830325,0.003641132261043236,0.003641107311164433,0.003641101254487104]|\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.003786634844210858,0.0037866312491918186,0.003786632809305149,0.0037866403006303786,0.004306346992543223,0.9654006242647946,0.003786608859072519,0.0037866401039978025,0.003786616736702421,0.0037866238395513996]|\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004303999453204036,0.004303978980090139,0.004304010799754381,0.004304107003705643,0.18445216321574132,0.781115595357762,0.004304013391658828,0.004304059430080946,0.0043040683232138215,0.004304004044788834]     |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004303937513791912,0.004303956646279543,0.004303991172029502,0.004304059250084989,0.960838023232403,0.004730053349850146,0.004303974103869076,0.004304000083517286,0.004304040266278829,0.004303964381895933]     |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.003263877283888617,0.003263882651709483,0.003263884116966922,0.003263858965718773,0.5001400922498412,0.473748890948236,0.0032638510091557915,0.0032638891445979116,0.003263844765253825,0.0032639288646316143]    |\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004116679427487175,0.004116640457922757,0.004116688188162257,0.004116765833001391,0.004683916536753829,0.9623824097381709,0.004116731473597529,0.004116779278096759,0.004116731901337986,0.004116657165469446]    |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.00473488141000836,0.0047348901700039695,0.004734921737818237,0.004734931707204489,0.9569185407755848,0.005202259140604683,0.004734903851507788,0.004734900336009495,0.004734888004245938,0.0047348828670123215]   |\n",
      "+-----+---------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(sample_lda_data_path)\n",
    "\n",
    "# TODO: Implement the remainder of the code from the LDA example\n",
    "\n",
    "# Code implimented from https://spark.apache.org/docs/latest/ml-clustering.html#k-means \n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 9.3\n",
    "\n",
    "Run the PySpark version of the collaborative filtering example found in [Apache Spark's collaborative filtering documentation](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_movielens_ratings.txt` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt. Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "lines = spark.read.text(\"data/mllib/als/sample_movielens_data.txt\").rdd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.6521732989246491\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lines = spark.read.text(sample_movielens_data_path).rdd\n",
    "\n",
    "# TODO: Implement the remainder of the code from the collaborative filtering example\n",
    "\n",
    "# Code implimented from https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
